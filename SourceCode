# 기본 라이브러리 import

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib

# !pip install statistics
import statistics 

----------------------------------------------------------

데이터 소스 : https://www.kaggle.com/competitions/santander-customer-satisfaction/data

----------------------------------------------------------

# Data 경로 및 불러오기

import os
os.getcwd()

PATH = os.getcwd() + "/Practice/현업_6월분석코드/santander-customer-satisfaction/"
cust_df = pd.read_csv(PATH + "train_santander.csv")

----------------------------------------------------------

# 전처리 전 EDA : pandas_profiling

# !pip install pandas_profiling
import pandas_profiling
cust_df.profile_report()

----------------------------------------------------------

# OutLier 탐지 및 최빈값대체

mode_var3 = statistics.mode(cust_df['var3'])
cust_df['var3'].replace(-999999, mode_var3, inplace=True)
cust_df['var3'].value_counts()

----------------------------------------------------------

# 필요없는 칼럼 drop, X_train, y_train(label) 데이터 분리
try :
    cust_df.drop('ID', axis=1, inplace=True)
except:
    pass

X_features = cust_df.iloc[:,:-1]
y_labels = cust_df.iloc[:,-1]

----------------------------------------------------------

# 교차검증(Cross Validation) 위한 데이터 쪼개기

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels,
                                                    test_size=0.2, random_state=0) #옵션: stratify = None -> 분포균일하게 섞기

----------------------------------------------------------

# 모델 성능 검증을 위한 함수 생성

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler, Binarizer
from sklearn.linear_model import LogisticRegression

import warnings
warnings.filterwarnings('ignore')


# get_clf_eval()
# 저번에 작성한 것에 ROC AUC를 추가함
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred_proba)
    
    print('오차행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'
          .format(accuracy, precision, recall, f1, roc_auc))
    
# get_eval_by_threshold()
def get_eval_by_threshold(y_test, pred_proba_c1, thresholds):
    # thresholds list 객체 내의 값을 차례로 iteration하면서 evaluation 수행
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1)
        custom_predict = binarizer.transform(pred_proba_c1)
        print(f'임곗값: {custom_threshold}')
        get_clf_eval(y_test, custom_predict)

# precision_recall_curve_plot()
def precision_recall_curve_plot(y_test, pred_proba_c1):
    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출
    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)
    
    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행, 정밀도는 점선으로 표시
    plt.figure(figsize=(8, 6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')
    
    # threshold 값 X축의 scale을 0.1 단위로 변경
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    
    # X, Y축 label과 legend, grid 설정
    plt.xlabel('Threshold value')
    plt.ylabel('Precision and Recall value')
    plt.legend()
    plt.grid()
    
----------------------------------------------------------

# XGBoosting학습(BaseModel)

from xgboost import XGBClassifier

xgb_clf = XGBClassifier(n_estimators=500, random_state=145)
xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=[(X_train, y_train),(X_test, y_test)])

preds = xgb_clf.predict(X_test)
pred_proba = xgb_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test, preds, pred_proba)

----------------------------------------------------------

# 임계값을 여러개로 조정하여 평가지표 조사
# 임계값
thresholds = [0.4, 0.45, 0.5, 0.55, 0.6]

# 평가지표를 조사하기 위한 새로운 함수 생성
def get_eval_by_threshold(y_test, pred_proba_c1, thresholds):
    #thresholds list 객체 내의 값을 iteration 하면서 평가 수행
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1)
        custom_predict = binarizer.transform(pred_proba_c1)
        print('\n임계값: ', custom_threshold)
        get_clf_eval(y_test, custom_predict, pred_proba)

get_eval_by_threshold(y_test, pred_proba.reshape(-1, 1), thresholds)

----------------------------------------------------------

get_clf_eval(y_test, preds, pred_proba)

----------------------------------------------------------

# 파라미터 튜닝

from sklearn.model_selection import GridSearchCV

xgb_clf = XGBClassifier(n_estimators=100)

params = {'max_depth':[5,7],
         'min_child_weight':[1,3],
         'colsample_bytree':[0.5,0.75]}

gridcv = GridSearchCV(xgb_clf, param_grid=params, cv=3)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc",
           eval_set=[(X_train, y_train),(X_test, y_test)])
           
print('GridSearchCV 최적 파라미터:', gridcv.best_params_)

xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))

----------------------------------------------------------

# 최적 파라미터로 돌리기

xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=5, min_child_seight=3,
                        colsample_bytree=0.5, reg_alpha=0.03)

xgb_clf.fit(X_train, y_train, early_stopping_rounds=200,
            eval_metric="auc", eval_set=[(X_train, y_train),(X_test,y_test)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predeic_proba(X_test)[:,1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))

----------------------------------------------------------

#각 피처 중요도 그래프 그리기
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(1,1,figsize=(10,8))
plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)

----------------------------------------------------------


----------------------------------------------------------

# LightGBM

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=500)

evals = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals, verbose=True)

----------------------------------------------------------

# 모델 예측 성능

preds = lgbm_clf.predict(X_test)
pred_proba = lgbm_clf.predict_proba(X_test)[:,1]
get_clf_eval(y_test, preds, pred_proba)

----------------------------------------------------------

# 파라미터 튜닝

from sklearn.model_selection import GridSearchCV

lgbm_clf = LGBMClassifier(n_estimators=200)

params = {'num_leaves':[32,64],
         'max+depth':[128,160],
         'min_child_samples':[60,100],
         'subsample':[0.8,1]}

gridcv = GridSearchCV(lgbm_clf, param_grid=params, cv=3)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc",
           eval_set=[(X_train, y_train), (X_test, y_test)])
           
----------------------------------------------------------

# girdCV결과 모델예측성능

print(gridcv.best_params_)
preds = gridcv.predict(X_test)
pred_proba = gridcv.predict_proba(X_test)[:,1]
print(get_clf_eval(y_test, preds, pred_proba)) 

----------------------------------------------------------

# 최적 파라미터로 튜닝후, n_estimators높여서 재학습

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, subsample=0.8, min_child_samples=100, max_depth=128)

evals = [(X_train, y_train),(X_test,y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals, verbose=True)

preds = lgbm_clf.predict(X_test)
pred_proba = lgbm_clf.predict_proba(X_test)[:,1]
get_clf_eval(y_test, preds, pred_proba)

----------------------------------------------------------


----------------------------------------------------------


----------------------------------------------------------


----------------------------------------------------------

