# 기본 라이브러리 import

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
%matplotlib inline
import warnings
warnings.filterwarnings("ignore")

# 오버샘플링(SMOTE)
# !pip install imbalanced-learn

#---------------------------------------------------------------

데이터 링크 : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

#---------------------------------------------------------------

# Data 경로 및 불러오기

import os
os.getcwd()

PATH = os.getcwd() + "/Practice/현업_6월분석코드/creditcard_data/"
card_df = pd.read_csv(PATH + "creditcard.csv")

#---------------------------------------------------------------

card_df.info()
card_df.describe() # 기초통계량
card_df['Class'].value_counts() #Label
card_df.head(3)

#---------------------------------------------------------------

# Time칼럼 삭제 함수

def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time', axis=1, inplace=True)
    return df_copy

# train/test 데이터셋 반환 함수
from sklearn.model_selection import train_test_split
def get_train_test_dataset(df=None):
    df_copy= get_preprocessed_df(df)
    X_features =df_copy.iloc[:,:-1]
    y_target = df_copy.iloc[:,-1]
    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

#---------------------------------------------------------------

# train/test 레이블 분류 비율 맞는지 Check

print(y_train.value_counts()[1]/y_train.value_counts()[0] *100,
      y_test.value_counts()[1]/y_test.value_counts()[0] *100)
      
#---------------------------------------------------------------

# 모델 성능 검증을 위한 함수 생성

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler, Binarizer
from sklearn.linear_model import LogisticRegression

import warnings
warnings.filterwarnings('ignore')

# get_clf_eval()
# 저번에 작성한 것에 ROC AUC를 추가함
def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred_proba)
    
    print('오차행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'
          .format(accuracy, precision, recall, f1, roc_auc))
          
#---------------------------------------------------------------

# 로지스틱 회귀분석

from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
lr_pred_proba = lr_clf.predict_proba(X_test)[:,1]

get_clf_eval(y_test, lr_pred, lr_pred_proba)

#---------------------------------------------------------------

# 인자로 사이킷런의 Estimator객체와 학습/테스트 데이터 샛을 입력 받아서 학습/예측/평가 수행

def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    pred_proba = model.predict_proba(ftr_test)[:,1]
    get_clf_eval(tgt_test, pred, pred_proba)
    
#---------------------------------------------------------------

# LightGBM

from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=100, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

#---------------------------------------------------------------

# 데이터 분포 변환

# Amount(결제금액) 변수  분포 시각화
# !pip install seaborn
import seaborn as sns
plt.figure(figsize=(8,4))
plt.xticks(range(0,30000,1000),rotation=60)
sns.distplot(card_df['Amount'])

#---------------------------------------------------------------

from sklearn.preprocessing import StandardScaler

def get_prprocessed_df(df):
    df_copy = df.copy()
    scaler = StandardScaler() #정규분포 변환
    amount_n =scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
    # 변환된 Amount를 Amount_Scaled로 피처면 변경호 DataFrame맨 앞 칼럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    # 기존 Time, Amount 피처 삭제
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    return df_copy
    
#---------------------------------------------------------------

# Amount를 정규분포로 변환 후 로지스틱회귀 & LightGBM 재실행

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, X_train, X_test, y_train, y_test)

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

#---------------------------------------------------------------

# Amount를 로그변환 후 로지스틱회귀 & LightGBM 재실행

def get_preprocessed_df(df):
    df_copy = df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    # 변환된 Amount를 Amount_Scaled로 피처면 변경호 DataFrame맨 앞 칼럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    # 기존 Time, Amount 피처 삭제
    df_copy.drop(['Time','Amount'], axis=1, inplace=True)
    return df_copy
    
#---------------------------------------------------------------

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, X_train, X_test, y_train, y_test)

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

#---------------------------------------------------------------

# 이상치 제거 후 모델 학습/예측 평가

# 각 피처별 상관관계 시각화
import seaborn as sns
plt.figure(figsize=(9,9))
corr=card_df.corr()
sns.heatmap(corr, cmap='RdBu')

#---------------------------------------------------------------

# IQR(사분위수 이용한 범위) 기준 Outlier 제거 함수

def get_outlier0(df, column, weight=1.5):
    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함.
    fraud = df[df['Class']==0][column]
    quantile_25 = np.percentile(fraud.values, 25)
    quantile_75 = np.percentile(fraud.values, 75)
    # IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최소값 지점 구함
    iqr = quantile_75 - quantile_25
    iqr_weight = iqr * weight
    lowest_val = quantile_25 - iqr_weight
    highest_val = quantile_75 + iqr_weight
    # 최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index 반환,
    outlier_index = fraud[(fraud < lowest_val) | (fraud>highest_val)].index
    return outlier_index

def get_outlier1(df, column, weight=1.5):
    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함.
    fraud = df[df['Class']==1][column]
    quantile_25 = np.percentile(fraud.values, 25)
    quantile_75 = np.percentile(fraud.values, 75)
    # IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최소값 지점 구함
    iqr = quantile_75 - quantile_25
    iqr_weight = iqr * weight
    lowest_val = quantile_25 - iqr_weight
    highest_val = quantile_75 + iqr_weight
    # 최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index 반환,
    outlier_index = fraud[(fraud < lowest_val) | (fraud>highest_val)].index
    return outlier_index

def get_outlier(df, column, weight=1.5):
    cl0_outlier = get_outlier0(df, column, weight=1.5)
    cl1_outlier = get_outlier1(df, column, weight=1.5)
    return cl0_outlier.union(cl1_outlier)


outlier_index = get_outlier(card_df, 'V14', weight=1.5)
outlier_index

#---------------------------------------------------------------

# Class 0 / Class 1 각각의 이상치 모두 제거한 경우의 결과

def get_preprocessed_df(df):
    df_copy = df.copy()
    # 로그변환
    amount_n = np.log1p(df_copy['Amount']) 
    df_copy.insert(0, 'Anount_Scaled', amount_n)
    # 이상치 데이터 삭제하는 로직 추가
    outlier_index = get_outlier(df_copy, 'V14', weight=1.5) #함수 get_outler 사용
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print("\n")
print("Class 0 / Class 1 각각의 이상치 모두 제거한 경우의 결과")
print("로지스틱")
get_model_train_eval(lr_clf, X_train, X_test, y_train, y_test)
print("LightGBM")
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

'''
# Class 1 일떄의 이상치만 제거한 경우의 결과

def get_preprocessed_df(df):
    df_copy = df.copy()
    # 로그변환
    amount_n = np.log1p(df_copy['Amount']) 
    df_copy.insert(0, 'Anount_Scaled', amount_n)
    # 이상치 데이터 삭제하는 로직 추가
    outlier_index = get_outlier1(df_copy, 'V14', weight=1.5) #함수 get_outler1 사용
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print("\n")
print("Class 1 일때의 이상치만 제거한 경우의 결과")
print("로지스틱")
get_model_train_eval(lr_clf, X_train, X_test, y_train, y_test)
print("LightGBM")
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

# Class 0 일떄의 이상치만 제거한 경우의 결과

def get_preprocessed_df(df):
    df_copy = df.copy()
    # 로그변환
    amount_n = np.log1p(df_copy['Amount']) 
    df_copy.insert(0, 'Anount_Scaled', amount_n)
    # 이상치 데이터 삭제하는 로직 추가
    outlier_index = get_outlier0(df_copy, 'V14', weight=1.5) #함수 get_outler0 사용
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print("\n")
print("Class 0 일때의 이상치만 제거한 경우의 결과")
print("로지스틱")
get_model_train_eval(lr_clf, X_train, X_test, y_train, y_test)
print("LightGBM")
get_model_train_eval(lgbm_clf, X_train, X_test, y_train, y_test)

'''

#---------------------------------------------------------------

# SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가
# !pip install imblearn
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train) #책이랑 다름, resample이 맞음
print(X_train.shape, y_train.shape)
print(X_train_over.shape, y_train_over.shape)
print(y_train_over.value_counts())

#---------------------------------------------------------------

# SMOTE이후 모델성능평가

print("로지스틱")
get_model_train_eval(lr_clf, X_train_over, X_test, y_train_over, y_test)
print("LightGBM")
get_model_train_eval(lgbm_clf, X_train_over, X_test, y_train_over, y_test)

print("SMOTE 사용시 재현율은 높아지나 정밀도는 낮아짐")

#---------------------------------------------------------------
